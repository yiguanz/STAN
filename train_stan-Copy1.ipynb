{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from epiweeks import Week\n",
    "\n",
    "from data_downloader import GenerateTrainingData\n",
    "from utils import date_today, gravity_law_commute_dist\n",
    "\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '16'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '8'\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from model import STAN\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish download\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>fips</th>\n",
       "      <th>date_today</th>\n",
       "      <th>confirmed</th>\n",
       "      <th>deaths</th>\n",
       "      <th>recovered</th>\n",
       "      <th>active</th>\n",
       "      <th>hospitalization</th>\n",
       "      <th>new_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>32.3182</td>\n",
       "      <td>-86.9023</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>8203</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7814.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>32.3182</td>\n",
       "      <td>-86.9023</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-05-05</td>\n",
       "      <td>8520</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8122.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>32.3182</td>\n",
       "      <td>-86.9023</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-05-06</td>\n",
       "      <td>8769</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8348.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>32.3182</td>\n",
       "      <td>-86.9023</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-05-07</td>\n",
       "      <td>9115</td>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8677.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>32.3182</td>\n",
       "      <td>-86.9023</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-05-08</td>\n",
       "      <td>9437</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9002.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>42.7560</td>\n",
       "      <td>-107.3025</td>\n",
       "      <td>56</td>\n",
       "      <td>2020-11-27</td>\n",
       "      <td>31773</td>\n",
       "      <td>0</td>\n",
       "      <td>21700.0</td>\n",
       "      <td>9858.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>42.7560</td>\n",
       "      <td>-107.3025</td>\n",
       "      <td>56</td>\n",
       "      <td>2020-11-28</td>\n",
       "      <td>31928</td>\n",
       "      <td>0</td>\n",
       "      <td>22798.0</td>\n",
       "      <td>8915.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>42.7560</td>\n",
       "      <td>-107.3025</td>\n",
       "      <td>56</td>\n",
       "      <td>2020-11-29</td>\n",
       "      <td>32489</td>\n",
       "      <td>0</td>\n",
       "      <td>23022.0</td>\n",
       "      <td>9252.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>42.7560</td>\n",
       "      <td>-107.3025</td>\n",
       "      <td>56</td>\n",
       "      <td>2020-11-30</td>\n",
       "      <td>33305</td>\n",
       "      <td>0</td>\n",
       "      <td>24478.0</td>\n",
       "      <td>8612.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>42.7560</td>\n",
       "      <td>-107.3025</td>\n",
       "      <td>56</td>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>33805</td>\n",
       "      <td>24</td>\n",
       "      <td>26003.0</td>\n",
       "      <td>7563.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12296 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      state  latitude  longitude  fips date_today  confirmed  deaths  \\\n",
       "0   Alabama   32.3182   -86.9023     1 2020-05-04       8203       8   \n",
       "0   Alabama   32.3182   -86.9023     1 2020-05-05       8520      17   \n",
       "0   Alabama   32.3182   -86.9023     1 2020-05-06       8769      28   \n",
       "0   Alabama   32.3182   -86.9023     1 2020-05-07       9115      26   \n",
       "0   Alabama   32.3182   -86.9023     1 2020-05-08       9437      14   \n",
       "..      ...       ...        ...   ...        ...        ...     ...   \n",
       "57  Wyoming   42.7560  -107.3025    56 2020-11-27      31773       0   \n",
       "57  Wyoming   42.7560  -107.3025    56 2020-11-28      31928       0   \n",
       "57  Wyoming   42.7560  -107.3025    56 2020-11-29      32489       0   \n",
       "57  Wyoming   42.7560  -107.3025    56 2020-11-30      33305       0   \n",
       "57  Wyoming   42.7560  -107.3025    56 2020-12-01      33805      24   \n",
       "\n",
       "    recovered  active  hospitalization  new_cases  \n",
       "0         0.0  7814.0              0.0        226  \n",
       "0         0.0  8122.0              0.0        317  \n",
       "0         0.0  8348.0              0.0        249  \n",
       "0         0.0  8677.0              0.0        346  \n",
       "0         0.0  9002.0              0.0        322  \n",
       "..        ...     ...              ...        ...  \n",
       "57    21700.0  9858.0              4.0       1012  \n",
       "57    22798.0  8915.0              4.0        155  \n",
       "57    23022.0  9252.0              2.0        561  \n",
       "57    24478.0  8612.0              0.0        816  \n",
       "57    26003.0  7563.0              2.0        500  \n",
       "\n",
       "[12296 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GenerateTrainingData().download_jhu_data('2020-05-01', '2020-12-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge population data with downloaded data\n",
    "raw_data = pickle.load(open('./data/state_covid_data.pickle','rb'))\n",
    "pop_data = pd.read_csv('./uszips.csv')\n",
    "pop_data = pop_data.groupby('state_name').agg({'population':'sum', 'density':'mean', 'lat':'mean', 'lng':'mean'}).reset_index()\n",
    "raw_data = pd.merge(raw_data, pop_data, how='inner', left_on='state', right_on='state_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>fips</th>\n",
       "      <th>date_today</th>\n",
       "      <th>confirmed</th>\n",
       "      <th>deaths</th>\n",
       "      <th>recovered</th>\n",
       "      <th>active</th>\n",
       "      <th>hospitalization</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>state_name</th>\n",
       "      <th>population</th>\n",
       "      <th>density</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11021</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>42.756</td>\n",
       "      <td>-107.3025</td>\n",
       "      <td>56</td>\n",
       "      <td>2020-11-29</td>\n",
       "      <td>32489</td>\n",
       "      <td>0</td>\n",
       "      <td>23022.0</td>\n",
       "      <td>9252.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>561</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>582091</td>\n",
       "      <td>90.858989</td>\n",
       "      <td>42.932801</td>\n",
       "      <td>-107.378368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11022</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>42.756</td>\n",
       "      <td>-107.3025</td>\n",
       "      <td>56</td>\n",
       "      <td>2020-11-30</td>\n",
       "      <td>33305</td>\n",
       "      <td>0</td>\n",
       "      <td>24478.0</td>\n",
       "      <td>8612.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>816</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>582091</td>\n",
       "      <td>90.858989</td>\n",
       "      <td>42.932801</td>\n",
       "      <td>-107.378368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11023</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>42.756</td>\n",
       "      <td>-107.3025</td>\n",
       "      <td>56</td>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>33805</td>\n",
       "      <td>24</td>\n",
       "      <td>26003.0</td>\n",
       "      <td>7563.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>500</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>582091</td>\n",
       "      <td>90.858989</td>\n",
       "      <td>42.932801</td>\n",
       "      <td>-107.378368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         state  latitude  longitude  fips date_today  confirmed  deaths  \\\n",
       "11021  Wyoming    42.756  -107.3025    56 2020-11-29      32489       0   \n",
       "11022  Wyoming    42.756  -107.3025    56 2020-11-30      33305       0   \n",
       "11023  Wyoming    42.756  -107.3025    56 2020-12-01      33805      24   \n",
       "\n",
       "       recovered  active  hospitalization  new_cases state_name  population  \\\n",
       "11021    23022.0  9252.0              2.0        561    Wyoming      582091   \n",
       "11022    24478.0  8612.0              0.0        816    Wyoming      582091   \n",
       "11023    26003.0  7563.0              2.0        500    Wyoming      582091   \n",
       "\n",
       "         density        lat         lng  \n",
       "11021  90.858989  42.932801 -107.378368  \n",
       "11022  90.858989  42.932801 -107.378368  \n",
       "11023  90.858989  42.932801 -107.378368  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate location similarity\n",
    "loc_list = list(raw_data['state'].unique())\n",
    "loc_dist_map = {}\n",
    "\n",
    "for each_loc in loc_list:\n",
    "    loc_dist_map[each_loc] = {}\n",
    "    for each_loc2 in loc_list:\n",
    "        lat1 = raw_data[raw_data['state']==each_loc]['latitude'].unique()[0]\n",
    "        lng1 = raw_data[raw_data['state']==each_loc]['longitude'].unique()[0]\n",
    "        pop1 = raw_data[raw_data['state']==each_loc]['population'].unique()[0]\n",
    "        \n",
    "        lat2 = raw_data[raw_data['state']==each_loc2]['latitude'].unique()[0]\n",
    "        lng2 = raw_data[raw_data['state']==each_loc2]['longitude'].unique()[0]\n",
    "        pop2 = raw_data[raw_data['state']==each_loc2]['population'].unique()[0]\n",
    "        \n",
    "        loc_dist_map[each_loc][each_loc2] = gravity_law_commute_dist(lat1, lng1, pop1, lat2, lng2, pop2, r=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Graph\n",
    "dist_threshold = 18\n",
    "\n",
    "for each_loc in loc_dist_map:\n",
    "    loc_dist_map[each_loc] = {k: v for k, v in sorted(loc_dist_map[each_loc].items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "adj_map = {}\n",
    "for each_loc in loc_dist_map:\n",
    "    adj_map[each_loc] = []\n",
    "    for i, each_loc2 in enumerate(loc_dist_map[each_loc]):\n",
    "        if loc_dist_map[each_loc][each_loc2] > dist_threshold:\n",
    "            if i <= 3:\n",
    "                adj_map[each_loc].append(each_loc2)\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            if i <= 1:\n",
    "                adj_map[each_loc].append(each_loc2)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "rows = []\n",
    "cols = []\n",
    "for each_loc in adj_map:\n",
    "    for each_loc2 in adj_map[each_loc]:\n",
    "        rows.append(loc_list.index(each_loc))\n",
    "        cols.append(loc_list.index(each_loc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.graph((rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess features\n",
    "\n",
    "active_cases = []\n",
    "confirmed_cases = []\n",
    "new_cases = []\n",
    "death_cases = []\n",
    "static_feat = []\n",
    "\n",
    "for i, each_loc in enumerate(loc_list):\n",
    "    active_cases.append(raw_data[raw_data['state'] == each_loc]['active'])\n",
    "    confirmed_cases.append(raw_data[raw_data['state'] == each_loc]['confirmed'])\n",
    "    new_cases.append(raw_data[raw_data['state'] == each_loc]['new_cases'])\n",
    "    death_cases.append(raw_data[raw_data['state'] == each_loc]['deaths'])\n",
    "    static_feat.append(np.array(raw_data[raw_data['state'] == each_loc][['population','density','lng','lat']]))\n",
    "    \n",
    "active_cases = np.array(active_cases)\n",
    "confirmed_cases = np.array(confirmed_cases)\n",
    "death_cases = np.array(death_cases)\n",
    "new_cases = np.array(new_cases)\n",
    "static_feat = np.array(static_feat)[:, 0, :]\n",
    "recovered_cases = confirmed_cases - active_cases - death_cases\n",
    "susceptible_cases = np.expand_dims(static_feat[:, 0], -1) - active_cases - recovered_cases\n",
    "\n",
    "# Batch_feat: new_cases(dI), dR, dS\n",
    "#dI = np.array(new_cases)\n",
    "dI = np.concatenate((np.zeros((active_cases.shape[0],1), dtype=np.float32), np.diff(active_cases)), axis=-1)\n",
    "dR = np.concatenate((np.zeros((recovered_cases.shape[0],1), dtype=np.float32), np.diff(recovered_cases)), axis=-1)\n",
    "dS = np.concatenate((np.zeros((susceptible_cases.shape[0],1), dtype=np.float32), np.diff(susceptible_cases)), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build normalizer\n",
    "normalizer = {'S':{}, 'I':{}, 'R':{}, 'dS':{}, 'dI':{}, 'dR':{}}\n",
    "\n",
    "for i, each_loc in enumerate(loc_list):\n",
    "    normalizer['S'][each_loc] = (np.mean(susceptible_cases[i]), np.std(susceptible_cases[i]))\n",
    "    normalizer['I'][each_loc] = (np.mean(active_cases[i]), np.std(active_cases[i]))\n",
    "    normalizer['R'][each_loc] = (np.mean(recovered_cases[i]), np.std(recovered_cases[i]))\n",
    "    normalizer['dI'][each_loc] = (np.mean(dI[i]), np.std(dI[i]))\n",
    "    normalizer['dR'][each_loc] = (np.mean(dR[i]), np.std(dR[i]))\n",
    "    normalizer['dS'][each_loc] = (np.mean(dS[i]), np.std(dS[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, sum_I, sum_R, history_window=5, pred_window=15, slide_step=5):\n",
    "    # Data shape n_loc, timestep, n_feat\n",
    "    # Reshape to n_loc, t, history_window*n_feat\n",
    "    n_loc = data.shape[0]\n",
    "    timestep = data.shape[1]\n",
    "    n_feat = data.shape[2]\n",
    "    \n",
    "    x = []\n",
    "    y_I = []\n",
    "    y_R = []\n",
    "    last_I = []\n",
    "    last_R = []\n",
    "    concat_I = []\n",
    "    concat_R = []\n",
    "    for i in range(0, timestep, slide_step):\n",
    "        if i+history_window+pred_window-1 >= timestep or i+history_window >= timestep:\n",
    "            break\n",
    "        x.append(data[:, i:i+history_window, :].reshape((n_loc, history_window*n_feat)))\n",
    "        \n",
    "        concat_I.append(data[:, i+history_window-1, 0])\n",
    "        concat_R.append(data[:, i+history_window-1, 1])\n",
    "        last_I.append(sum_I[:, i+history_window-1])\n",
    "        last_R.append(sum_R[:, i+history_window-1])\n",
    "\n",
    "        y_I.append(data[:, i+history_window:i+history_window+pred_window, 0])\n",
    "        y_R.append(data[:, i+history_window:i+history_window+pred_window, 1])\n",
    "\n",
    "    x = np.array(x, dtype=np.float32).transpose((1, 0, 2))\n",
    "    last_I = np.array(last_I, dtype=np.float32).transpose((1, 0))\n",
    "    last_R = np.array(last_R, dtype=np.float32).transpose((1, 0))\n",
    "    concat_I = np.array(concat_I, dtype=np.float32).transpose((1, 0))\n",
    "    concat_R = np.array(concat_R, dtype=np.float32).transpose((1, 0))\n",
    "    y_I = np.array(y_I, dtype=np.float32).transpose((1, 0, 2))\n",
    "    y_R = np.array(y_R, dtype=np.float32).transpose((1, 0, 2))\n",
    "    return x, last_I, last_R, concat_I, concat_R, y_I, y_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_window = 25\n",
    "test_window = 25\n",
    "\n",
    "history_window=6\n",
    "pred_window=15\n",
    "slide_step=5\n",
    "\n",
    "dynamic_feat = np.concatenate((np.expand_dims(dI, axis=-1), np.expand_dims(dR, axis=-1), np.expand_dims(dS, axis=-1)), axis=-1)\n",
    "    \n",
    "#Normalize\n",
    "for i, each_loc in enumerate(loc_list):\n",
    "    dynamic_feat[i, :, 0] = (dynamic_feat[i, :, 0] - normalizer['dI'][each_loc][0]) / normalizer['dI'][each_loc][1]\n",
    "    dynamic_feat[i, :, 1] = (dynamic_feat[i, :, 1] - normalizer['dR'][each_loc][0]) / normalizer['dR'][each_loc][1]\n",
    "    dynamic_feat[i, :, 2] = (dynamic_feat[i, :, 2] - normalizer['dS'][each_loc][0]) / normalizer['dS'][each_loc][1]\n",
    "\n",
    "dI_mean = []\n",
    "dI_std = []\n",
    "dR_mean = []\n",
    "dR_std = []\n",
    "\n",
    "for i, each_loc in enumerate(loc_list):\n",
    "    dI_mean.append(normalizer['dI'][each_loc][0])\n",
    "    dR_mean.append(normalizer['dR'][each_loc][0])\n",
    "    dI_std.append(normalizer['dI'][each_loc][1])\n",
    "    dR_std.append(normalizer['dR'][each_loc][1])\n",
    "\n",
    "dI_mean = np.array(dI_mean)\n",
    "dI_std = np.array(dI_std)\n",
    "dR_mean = np.array(dR_mean)\n",
    "dR_std = np.array(dR_std)\n",
    "\n",
    "#Split train-test\n",
    "train_feat = dynamic_feat[:, :-valid_window-test_window, :]\n",
    "val_feat = dynamic_feat[:, -valid_window-test_window:-test_window, :]\n",
    "test_feat = dynamic_feat[:, -test_window:, :]\n",
    "\n",
    "train_x, train_I, train_R, train_cI, train_cR, train_yI, train_yR = prepare_data(train_feat, active_cases[:, :-valid_window-test_window], recovered_cases[:, :-valid_window-test_window], history_window, pred_window, slide_step)\n",
    "val_x, val_I, val_R, val_cI, val_cR, val_yI, val_yR = prepare_data(val_feat, active_cases[:, -valid_window-test_window:-test_window], recovered_cases[:, -valid_window-test_window:-test_window], history_window, pred_window, slide_step)\n",
    "test_x, test_I, test_R, test_cI, test_cR, test_yI, test_yR = prepare_data(test_feat, active_cases[:, -test_window:], recovered_cases[:, -test_window:], history_window, pred_window, slide_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 212, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Build STAN model\n",
    "\n",
    "in_dim = 3*history_window\n",
    "hidden_dim1 = 32\n",
    "hidden_dim2 = 32\n",
    "gru_dim = 32\n",
    "num_heads = 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "g = g.to(device)\n",
    "model = STAN(g, in_dim, hidden_dim1, hidden_dim2, gru_dim, num_heads, pred_window, device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STAN(\n",
       "  (layer1): MultiHeadGATLayer(\n",
       "    (heads): ModuleList(\n",
       "      (0): GATLayer(\n",
       "        (fc): Linear(in_features=18, out_features=32, bias=True)\n",
       "        (attn_fc): Linear(in_features=64, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): MultiHeadGATLayer(\n",
       "    (heads): ModuleList(\n",
       "      (0): GATLayer(\n",
       "        (fc): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (attn_fc): Linear(in_features=64, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gru): GRUCell(32, 32)\n",
       "  (nn_res_I): Linear(in_features=34, out_features=15, bias=True)\n",
       "  (nn_res_R): Linear(in_features=34, out_features=15, bias=True)\n",
       "  (nn_res_sir): Linear(in_features=34, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor(train_x).to(device)\n",
    "train_I = torch.tensor(train_I).to(device)\n",
    "train_R = torch.tensor(train_R).to(device)\n",
    "train_cI = torch.tensor(train_cI).to(device)\n",
    "train_cR = torch.tensor(train_cR).to(device)\n",
    "train_yI = torch.tensor(train_yI).to(device)\n",
    "train_yR = torch.tensor(train_yR).to(device)\n",
    "\n",
    "val_x = torch.tensor(val_x).to(device)\n",
    "val_I = torch.tensor(val_I).to(device)\n",
    "val_R = torch.tensor(val_R).to(device)\n",
    "val_cI = torch.tensor(val_cI).to(device)\n",
    "val_cR = torch.tensor(val_cR).to(device)\n",
    "val_yI = torch.tensor(val_yI).to(device)\n",
    "val_yR = torch.tensor(val_yR).to(device)\n",
    "\n",
    "test_x = torch.tensor(test_x).to(device)\n",
    "test_I = torch.tensor(test_I).to(device)\n",
    "test_R = torch.tensor(test_R).to(device)\n",
    "test_cI = torch.tensor(test_cI).to(device)\n",
    "test_cR = torch.tensor(test_cR).to(device)\n",
    "test_yI = torch.tensor(test_yI).to(device)\n",
    "test_yR = torch.tensor(test_yR).to(device)\n",
    "\n",
    "dI_mean = torch.tensor(dI_mean, dtype=torch.float32).to(device).reshape((dI_mean.shape[0], 1, 1))\n",
    "dI_std = torch.tensor(dI_std, dtype=torch.float32).to(device).reshape((dI_mean.shape[0], 1, 1))\n",
    "dR_mean = torch.tensor(dR_mean, dtype=torch.float32).to(device).reshape((dI_mean.shape[0], 1, 1))\n",
    "dR_std = torch.tensor(dR_std, dtype=torch.float32).to(device).reshape((dI_mean.shape[0], 1, 1))\n",
    "\n",
    "N = torch.tensor(static_feat[:, 0], dtype=torch.float32).to(device).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_real_y(data, history_window=5, pred_window=15, slide_step=5):\n",
    "    # Data shape n_loc, timestep, n_feat\n",
    "    # Reshape to n_loc, t, history_window*n_feat\n",
    "    n_loc = data.shape[0]\n",
    "    timestep = data.shape[1]\n",
    "    \n",
    "    y = []\n",
    "    for i in range(0, timestep, slide_step):\n",
    "        if i+history_window+pred_window-1 >= timestep or i+history_window >= timestep:\n",
    "            break\n",
    "        y.append(data[:, i+history_window:i+history_window+pred_window])\n",
    "    y = np.array(y, dtype=np.float32).transpose((1, 0, 2))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_true = get_real_y(active_cases[:], history_window, pred_window, slide_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "previous_day_mse = 0\n",
    "predicted_mse = 0\n",
    "\n",
    "for i in range(52):\n",
    "    in_dim = 3*history_window\n",
    "    hidden_dim1 = 32\n",
    "    hidden_dim2 = 32\n",
    "    gru_dim = 32\n",
    "    num_heads = 1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    g = g.to(device)\n",
    "    model = STAN(g, in_dim, hidden_dim1, hidden_dim2, gru_dim, num_heads, pred_window, device).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    all_loss = []\n",
    "    file_name = './save/stan'\n",
    "    min_loss = 1e10\n",
    "\n",
    "    loc_name = loc_list[i]\n",
    "    cur_loc = i\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        active_pred, recovered_pred, phy_active, phy_recover, _ = model(train_x, train_cI[cur_loc], train_cR[cur_loc], N[cur_loc], train_I[cur_loc], train_R[cur_loc])\n",
    "        phy_active = (phy_active - dI_mean[cur_loc]) / dI_std[cur_loc]\n",
    "        phy_recover = (phy_recover - dR_mean[cur_loc]) / dR_std[cur_loc]\n",
    "        loss = criterion(active_pred.squeeze(), train_yI[cur_loc])+criterion(recovered_pred.squeeze(), train_yR[cur_loc])+0.1*criterion(phy_active.squeeze(), train_yI[cur_loc])+0.1*criterion(phy_recover.squeeze(), train_yR[cur_loc])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_loss.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        print(train_x.shape)\n",
    "        print(train_cI[cur_loc].shape)\n",
    "        print(train_cR[cur_loc].shape)\n",
    "        print(N[cur_loc])\n",
    "        _, _, _, _, prev_h = model(train_x, train_cI[cur_loc], train_cR[cur_loc], N[cur_loc], train_I[cur_loc], train_R[cur_loc])\n",
    "        val_active_pred, val_recovered_pred, val_phy_active, val_phy_recover, _ = model(val_x, val_cI[cur_loc], val_cR[cur_loc], N[cur_loc], val_I[cur_loc], val_R[cur_loc], prev_h)\n",
    "\n",
    "        val_phy_active = (val_phy_active - dI_mean[cur_loc]) / dI_std[cur_loc]\n",
    "        val_loss = criterion(val_active_pred.squeeze(), val_yI[cur_loc]) + 0.1*criterion(val_phy_active.squeeze(), val_yI[cur_loc])\n",
    "        if val_loss < min_loss:    \n",
    "            state = {\n",
    "                'state': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(state, file_name)\n",
    "            min_loss = val_loss\n",
    "            print('-----Save best model-----')\n",
    "\n",
    "        print('Epoch %d, Loss %.2f, Val loss %.2f'%(epoch, all_loss[-1], val_loss.item()))\n",
    "        \n",
    "    file_name = './save/stan'\n",
    "    checkpoint = torch.load(file_name)\n",
    "    model.load_state_dict(checkpoint['state'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    prev_x = torch.cat((train_x, val_x), dim=1)\n",
    "    prev_I = torch.cat((train_I, val_I), dim=1)\n",
    "    prev_R = torch.cat((train_R, val_R), dim=1)\n",
    "    prev_cI = torch.cat((train_cI, val_cI), dim=1)\n",
    "    prev_cR = torch.cat((train_cR, val_cR), dim=1)\n",
    "    prev_active_pred, _, prev_phyactive_pred, _, h = model(prev_x, prev_cI[cur_loc], prev_cR[cur_loc], N[cur_loc], prev_I[cur_loc], prev_R[cur_loc])\n",
    "\n",
    "    test_pred_active, test_pred_recovered, test_pred_phy_active, test_pred_phy_recover, _ = model(test_x, test_cI[cur_loc], test_cR[cur_loc], N[cur_loc], test_I[cur_loc], test_R[cur_loc], h)\n",
    "\n",
    "    pred_I = []\n",
    "\n",
    "    for i in range(test_pred_active.size(1)):\n",
    "        cur_pred = (test_pred_active[0, i, :].detach().cpu().numpy() * dI_std[cur_loc].reshape(1, 1).detach().cpu().numpy()) + dI_mean[cur_loc].reshape(1, 1).detach().cpu().numpy()\n",
    "        #cur_pred = test_pred_phy_active[0, i, :].detach().cpu().numpy()\n",
    "        cur_pred = (cur_pred + test_pred_phy_active[0, i, :].detach().cpu().numpy()) / 2\n",
    "        cur_pred = np.cumsum(cur_pred)\n",
    "        cur_pred = cur_pred + test_I[cur_loc, i].detach().cpu().item()\n",
    "        pred_I.append(cur_pred)\n",
    "    pred_I = np.array(pred_I)\n",
    "    pred_I = pred_I\n",
    "    \n",
    "    ground_truth = np.array(list(I_true[cur_loc, -1, :]))\n",
    "    previous_day = np.array(list([I_true[cur_loc, -2, 4]]) + list(I_true[cur_loc, -1, :-1]))\n",
    "    predicted = np.array(list(pred_I[-1, :]))\n",
    "    \n",
    "    plt.plot(list(range(15)), ground_truth,c='r', label='Ground truth')\n",
    "    plt.plot(list(range(15)), previous_day,c='g', label='Previous day')\n",
    "    plt.plot(list(range(15)), predicted,c='b', label='STAN')\n",
    "    plt.title(loc_name)\n",
    "    plt.legend()\n",
    "    plt.savefig('images/'+str(title)+'.png')\n",
    "    plt.cla()\n",
    "    \n",
    "    predicted_mse += ((ground_truth - previous_day) **2).mean()\n",
    "    previous_day_mse += ((ground_truth - predicted) **2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GATSIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, g, input_size, hidden_size, output_size, gcn_nlayers, num_heads=5):\n",
    "        super(GAT, self).__init__()\n",
    "        self.g = g\n",
    "        self.gcn_layers = nn.ModuleList()\n",
    "        self.gcn_layers.append(dglnn.conv.GATConv(input_size, hidden_size, num_heads=num_heads,\n",
    "                                                  residual=True, activation=F.relu))\n",
    "        for i in range(gcn_nlayers):\n",
    "            self.gcn_layers.append(dglnn.conv.GATConv(num_heads * hidden_size, hidden_size, num_heads=num_heads,\n",
    "                                                      residual=True, activation=F.relu))\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(num_heads * hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for layer in self.gcn_layers:\n",
    "            h = layer(self.g, h).flatten(1)\n",
    "        return self.linear_layers(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import pytorch as dglnn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class GAT_SIR(nn.Module):\n",
    "    def __init__(self, g, input_size, hidden_size, output_size, gcn_nlayers, num_heads=5):\n",
    "        super(GAT_SIR, self).__init__()\n",
    "        self.g = g\n",
    "        self.gcn_layers = nn.ModuleList()\n",
    "        self.gcn_layers.append(dglnn.conv.GATConv(input_size, hidden_size, num_heads=num_heads,\n",
    "                                                  residual=True, activation=F.relu))\n",
    "        for i in range(gcn_nlayers):\n",
    "            self.gcn_layers.append(dglnn.conv.GATConv(num_heads * hidden_size, hidden_size, num_heads=num_heads,\n",
    "                                                      residual=True, activation=F.relu))\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(num_heads * hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for layer in self.gcn_layers:\n",
    "            h = layer(self.g, h).flatten(1)\n",
    "        return self.linear_layers(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'th' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8e266d4cd83f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mnet_gat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAT_SIR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_past_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mtrain_past_mobility\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'th' is not defined"
     ]
    }
   ],
   "source": [
    "hidden_size = 100\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 500\n",
    "batch_size = 256\n",
    "output_size = 2\n",
    "\n",
    "device = th.device('cuda:0')\n",
    "net_gat = GAT_SIR(g, train_past_history[0].shape[1] +train_past_mobility[0].shape[1]+ attrs.shape[1], hidden_size, output_size, 1)\n",
    "\n",
    "# Move model to GPU\n",
    "net_gat = net_gat.to(device)\n",
    "\n",
    "criteria = nn.MSELoss()\n",
    "\n",
    "# Move data to GPU\n",
    "pop = pop.to(device)\n",
    "attrs = attrs.to(device)\n",
    "attrs[th.isnan(attrs)] = 0\n",
    "for i in range(len(train_past_cases)):\n",
    "    train_past_cases[i] = train_past_cases[i].to(device)\n",
    "    train_past_deaths[i] = train_past_deaths[i].to(device)\n",
    "    train_past_combined[i] = train_past_combined[i].to(device)\n",
    "    train_past_history[i] = train_past_history[i].to(device)\n",
    "    train_past_mobility[i] = train_past_mobility[i].to(device)\n",
    "    train_labels_cases[i] = train_labels_cases[i].to(device)\n",
    "    train_labels_deaths[i] = train_labels_deaths[i].to(device)\n",
    "    if len(train_labels_cases[i].shape) == 1:\n",
    "        train_labels_cases[i] = train_labels_cases[i].unsqueeze(1)\n",
    "    if len(train_labels_deaths[i].shape) == 1:\n",
    "        train_labels_deaths[i] = train_labels_deaths[i].unsqueeze(1)\n",
    "        \n",
    "for i in range(len(valid_past_cases)):\n",
    "    valid_past_cases[i] = valid_past_cases[i].to(device)\n",
    "    valid_past_deaths[i] = valid_past_deaths[i].to(device)\n",
    "    valid_past_history[i] = valid_past_history[i].to(device)\n",
    "    valid_past_combined[i] = valid_past_combined[i].to(device)\n",
    "    valid_past_mobility[i] = valid_past_mobility[i].to(device)\n",
    "    valid_labels_cases[i] = valid_labels_cases[i].to(device)\n",
    "    valid_labels_deaths[i] = valid_labels_deaths[i].to(device)\n",
    "    if len(valid_labels_cases[i].shape) == 1:\n",
    "         valid_labels_cases[i] = valid_labels_cases[i].unsqueeze(1)\n",
    "    if len(test_labels_deaths[i].shape) == 1:\n",
    "        valid_labels_deaths[i] = valid_labels_deaths[i].unsqueeze(1)\n",
    "\n",
    "for i in range(len(test_past_cases)):\n",
    "    test_past_cases[i] = test_past_cases[i].to(device)\n",
    "    test_past_deaths[i] = test_past_deaths[i].to(device)\n",
    "    test_past_history[i] = test_past_history[i].to(device)\n",
    "    test_past_combined[i] = test_past_combined[i].to(device)\n",
    "    test_past_mobility[i] = test_past_mobility[i].to(device)\n",
    "    test_labels_cases[i] = test_labels_cases[i].to(device)\n",
    "    test_labels_deaths[i] = test_labels_deaths[i].to(device)\n",
    "    if len(test_labels_cases[i].shape) == 1:\n",
    "        test_labels_cases[i] = test_labels_cases[i].unsqueeze(1)\n",
    "    if len(test_labels_deaths[i].shape) == 1:\n",
    "        test_labels_deaths[i] = test_labels_deaths[i].unsqueeze(1)        \n",
    "\n",
    "attr_temp = attrs\n",
    "attr_mean = th.mean(attr_temp, dim=1, keepdim=True)\n",
    "attrs_norm = attr_temp - attr_mean\n",
    "attr_std = th.std(attr_temp, dim=1, keepdim=True)\n",
    "attrs_norm = attrs_norm / attr_std\n",
    "\n",
    "gat_optimizer = th.optim.Adam(net_gat.parameters(), lr=learning_rate)\n",
    "\n",
    "last_loss = 0\n",
    "for epoch in range(num_epochs):\n",
    "    sample_idxs = th.randperm(len(train_past_deaths))\n",
    "    losses = []   \n",
    "    real_losses = []\n",
    "    for idx in sample_idxs:\n",
    "        labels_cases = train_labels_cases[idx]\n",
    "        labels_deaths = train_labels_deaths[idx]\n",
    "        past_combined = th.cat([th.log(th.add(train_past_history[idx],1)), train_past_mobility[idx]], dim=1)\n",
    "        batch = th.cat([past_combined, attrs_norm], dim=1)\n",
    "        gat_optimizer.zero_grad() \n",
    "        vals = net_gat(batch)\n",
    "        I = train_past_cases[idx][:,-1].view(3142,1)\n",
    "        D = train_past_deaths[idx][:,-1].view(3142,1)\n",
    "        I_lst1, D_lst1= sir_1d_output(vals, I, D)    \n",
    "        I_lst2, D_lst2= sir_1d_output(vals, I_lst1, D_lst1) \n",
    "        I_lst3, D_lst3= sir_1d_output(vals, I_lst2, D_lst2)\n",
    "        I_lst4, D_lst4= sir_1d_output(vals, I_lst3, D_lst3)\n",
    "        I_lst5, D_lst5= sir_1d_output(vals, I_lst4, D_lst4) \n",
    "        I_lst6, D_lst6= sir_1d_output(vals, I_lst5, D_lst5) \n",
    "        I_lst7, D_lst7= sir_1d_output(vals, I_lst6, D_lst6) \n",
    "        loss1 = my_msle(D_lst1, labels_deaths[:,0].view(3142,1))\n",
    "        loss2 = my_msle(D_lst2, labels_deaths[:,1].view(3142,1))\n",
    "        loss3 = my_msle(D_lst3, labels_deaths[:,2].view(3142,1))\n",
    "        loss4 = my_msle(D_lst4, labels_deaths[:,3].view(3142,1))\n",
    "        loss5 = my_msle(D_lst5, labels_deaths[:,4].view(3142,1))\n",
    "        loss6 = my_msle(D_lst6, labels_deaths[:,5].view(3142,1))\n",
    "        loss7 = my_msle(D_lst7, labels_deaths[:,6].view(3142,1))\n",
    "        loss = loss1 + loss2 + loss3 + loss4 + loss5 + loss6 + loss7\n",
    "        loss.backward(retain_graph=True)\n",
    "        gat_optimizer.step()\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        real_losses.append(real_loss.detach().cpu().numpy())\n",
    "\n",
    "    with th.no_grad():\n",
    "        eval_mses = []\n",
    "        eval_msles = []\n",
    "        for idx in range(len(valid_past_deaths)):\n",
    "            eval_labels_cases = valid_labels_cases[idx]\n",
    "            eval_labels_deaths = valid_labels_deaths[idx]\n",
    "            eval_past_combined = th.cat([th.log(th.add(valid_past_history[idx],1)), valid_past_mobility[idx]], dim=1)\n",
    "            eval_batch = th.cat([eval_past_combined, attrs_norm], dim=1)\n",
    "            eval_vals = net_gat(eval_batch)\n",
    "            eval_I = valid_past_cases[idx][:,-1].view(3142,1)\n",
    "            eval_D = valid_past_deaths[idx][:,-1].view(3142,1)\n",
    "            eval_I_lst1, eval_D_lst1= sir_1d_output(eval_vals, eval_I, eval_D)    \n",
    "            eval_I_lst2, eval_D_lst2= sir_1d_output(eval_vals, eval_I_lst1, eval_D_lst1) \n",
    "            eval_I_lst3, eval_D_lst3= sir_1d_output(eval_vals, eval_I_lst2, eval_D_lst2)\n",
    "            eval_I_lst4, eval_D_lst4= sir_1d_output(eval_vals, eval_I_lst3, eval_D_lst3)\n",
    "            eval_I_lst5, eval_D_lst5= sir_1d_output(eval_vals, eval_I_lst4, eval_D_lst4) \n",
    "            eval_I_lst6, eval_D_lst6= sir_1d_output(eval_vals, eval_I_lst5, eval_D_lst5) \n",
    "            eval_I_lst7, eval_D_lst7= sir_1d_output(eval_vals, eval_I_lst6, eval_D_lst6) \n",
    "            eval_loss1 = criteria(eval_D_lst1, eval_labels_deaths[:,0].view(3142,1))\n",
    "            eval_loss2 = criteria(eval_D_lst2, eval_labels_deaths[:,1].view(3142,1))\n",
    "            eval_loss3 = criteria(eval_D_lst3, eval_labels_deaths[:,2].view(3142,1))\n",
    "            eval_loss4 = criteria(eval_D_lst4, eval_labels_deaths[:,3].view(3142,1))\n",
    "            eval_loss5 = criteria(eval_D_lst5, eval_labels_deaths[:,4].view(3142,1))\n",
    "            eval_loss6 = criteria(eval_D_lst6, eval_labels_deaths[:,5].view(3142,1))\n",
    "            eval_loss7 = criteria(eval_D_lst7, eval_labels_deaths[:,6].view(3142,1))\n",
    "            eval_loss = eval_loss1 + eval_loss2 + eval_loss3 + eval_loss4 + eval_loss5 + eval_loss6 + eval_loss7\n",
    "            eval_mses.append(eval_loss.cpu().numpy())\n",
    "            \n",
    "            eval_msle1 = my_msle(eval_D_lst1, eval_labels_deaths[:,0].view(3142,1))\n",
    "            eval_msle2 = my_msle(eval_D_lst2, eval_labels_deaths[:,1].view(3142,1))\n",
    "            eval_msle3 = my_msle(eval_D_lst3, eval_labels_deaths[:,2].view(3142,1))\n",
    "            eval_msle4 = my_msle(eval_D_lst4, eval_labels_deaths[:,3].view(3142,1))\n",
    "            eval_msle5 = my_msle(eval_D_lst5, eval_labels_deaths[:,4].view(3142,1))\n",
    "            eval_msle6 = my_msle(eval_D_lst6, eval_labels_deaths[:,5].view(3142,1))\n",
    "            eval_msle7 = my_msle(eval_D_lst7, eval_labels_deaths[:,6].view(3142,1))\n",
    "            eval_msle = eval_msle1 + eval_msle2 + eval_msle3 + eval_msle4 + eval_msle5 + eval_msle6 + eval_msle7\n",
    "            eval_msles.append(eval_msle.cpu().numpy())\n",
    "\n",
    "        test_err1 = []\n",
    "        test_err2 = []\n",
    "        test_err3 = []\n",
    "        test_err4 = []\n",
    "        test_err5 = []\n",
    "        test_err6 = []\n",
    "        test_err7 = []\n",
    "        test_msles = []\n",
    "        test_mses = []\n",
    "        test_maes = []\n",
    "        for idx in range(len(test_past_deaths)):\n",
    "            test_labels_d = test_labels_deaths[idx]\n",
    "            test_past_combined = th.cat([th.log(th.add(test_past_history[idx],1)), test_past_mobility[idx]], dim=1)\n",
    "            test_batch = th.cat([test_past_combined, attrs_norm], dim=1)\n",
    "            test_vals = net_gat(test_batch)\n",
    "            test_I = test_past_cases[idx][:,-1].view(3142,1)\n",
    "            test_D = test_past_deaths[idx][:,-1].view(3142,1)\n",
    "            test_I_lst1, test_D_lst1= sir_1d_output(test_vals, test_I, test_D)    \n",
    "            test_I_lst2, test_D_lst2= sir_1d_output(test_vals, test_I_lst1, test_D_lst1) \n",
    "            test_I_lst3, test_D_lst3= sir_1d_output(test_vals, test_I_lst2, test_D_lst2)\n",
    "            test_I_lst4, test_D_lst4= sir_1d_output(test_vals, test_I_lst3, test_D_lst3)\n",
    "            test_I_lst5, test_D_lst5= sir_1d_output(test_vals, test_I_lst4, test_D_lst4) \n",
    "            test_I_lst6, test_D_lst6= sir_1d_output(test_vals, test_I_lst5, test_D_lst5) \n",
    "            test_I_lst7, test_D_lst7= sir_1d_output(test_vals, test_I_lst6, test_D_lst6) \n",
    "            test_loss1 = criteria(test_D_lst1, test_labels_d[:,0].view(3142,1))\n",
    "            test_loss2 = criteria(test_D_lst2, test_labels_d[:,1].view(3142,1))\n",
    "            test_loss3 = criteria(test_D_lst3, test_labels_d[:,2].view(3142,1))\n",
    "            test_loss4 = criteria(test_D_lst4, test_labels_d[:,3].view(3142,1))\n",
    "            test_loss5 = criteria(test_D_lst5, test_labels_d[:,4].view(3142,1))\n",
    "            test_loss6 = criteria(test_D_lst6, test_labels_d[:,5].view(3142,1))\n",
    "            test_loss7 = criteria(test_D_lst7, test_labels_d[:,6].view(3142,1))\n",
    "            test_loss = test_loss1 + test_loss2 + test_loss3 + test_loss4 + test_loss5 + test_loss6 + test_loss7\n",
    "            test_mses.append(test_loss.cpu().numpy())\n",
    "            test_err1.append(test_loss1.cpu().numpy())\n",
    "            test_err2.append(test_loss2.cpu().numpy())\n",
    "            test_err3.append(test_loss3.cpu().numpy())\n",
    "            test_err4.append(test_loss4.cpu().numpy())\n",
    "            test_err5.append(test_loss5.cpu().numpy())\n",
    "            test_err6.append(test_loss6.cpu().numpy())\n",
    "            test_err7.append(test_loss7.cpu().numpy())\n",
    "            \n",
    "            test_msle1 = my_msle(test_D_lst1, test_labels_d[:,0].view(3142,1))\n",
    "            test_msle2 = my_msle(test_D_lst2, test_labels_d[:,1].view(3142,1))\n",
    "            test_msle3 = my_msle(test_D_lst3, test_labels_d[:,2].view(3142,1))\n",
    "            test_msle4 = my_msle(test_D_lst4, test_labels_d[:,3].view(3142,1))\n",
    "            test_msle5 = my_msle(test_D_lst5, test_labels_d[:,4].view(3142,1))\n",
    "            test_msle6 = my_msle(test_D_lst6, test_labels_d[:,5].view(3142,1))\n",
    "            test_msle7 = my_msle(test_D_lst7, test_labels_d[:,6].view(3142,1))\n",
    "            test_msle = test_msle1 + test_msle2 + test_msle3 + test_msle4 + test_msle5 + test_msle6 + test_msle7\n",
    "            test_msles.append(test_msle.cpu().numpy())\n",
    "            \n",
    "            test_mae1 = my_mae(test_D_lst1, test_labels_d[:,0].view(3142,1))\n",
    "            test_mae2 = my_mae(test_D_lst2, test_labels_d[:,1].view(3142,1))\n",
    "            test_mae3 = my_mae(test_D_lst3, test_labels_d[:,2].view(3142,1))\n",
    "            test_mae4 = my_mae(test_D_lst4, test_labels_d[:,3].view(3142,1))\n",
    "            test_mae5 = my_mae(test_D_lst5, test_labels_d[:,4].view(3142,1))\n",
    "            test_mae6 = my_mae(test_D_lst6, test_labels_d[:,5].view(3142,1))\n",
    "            test_mae7 = my_mae(test_D_lst7, test_labels_d[:,6].view(3142,1))\n",
    "            test_mae = test_mae1 + test_mae2 + test_mae3 + test_mae4 + test_mae5 + test_mae6 + test_mae7\n",
    "            test_maes.append(test_mae.cpu().numpy())\n",
    "            \n",
    "        print('epoch={}, loss={:.5f}, validation msle = {:.5f}, validation mse = {:.5f}, test msle={:.5f}, test mse = {:.3f}, test mae = {:.3f}'.format(epoch, np.mean(losses), np.mean(eval_msles),np.mean(eval_mses),np.mean(test_msles), np.mean(test_mses), np.mean(test_maes)))\n",
    "        print('day1={:.2f},day2={:.2f},day3={:.2f},day4={:.2f},day5={:.2f},day6={:.2f},day7={:.2f}'.format(np.mean(test_err1), np.mean(test_err2),np.mean(test_err3),np.mean(test_err4),np.mean(test_err5),np.mean(test_err6),np.mean(test_err7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
